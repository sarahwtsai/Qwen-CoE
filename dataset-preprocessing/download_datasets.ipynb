{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Q&A Datasets\n",
    "This notebook will download and prepare Q&A datasets for training a feed forward network/routing network. The following datasets will be prepared: \n",
    "- ARC AI2 Reasoning Challenge: 7,787 grade-school science questions\n",
    "- OpenBookQA: 5,957 science and reasoning questions\n",
    "- MathQA: ~37,000 math word problems\n",
    "- CommonsenseQA: 12,247 general reasoning questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, load_from_disk\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARC AI2 Reasoning Challenge\n",
    "\n",
    "https://huggingface.co/datasets/allenai/ai2_arc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7787/7787 [00:01<00:00, 6364.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 7787/7787 [00:00<00:00, 324998.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_arc1 = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\")\n",
    "ds_arc2 = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\")\n",
    "\n",
    "# Add the \"source\" field to each example\n",
    "def add_source_field(example, source_name):\n",
    "    example['source'] = source_name\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to each split in both datasets\n",
    "ds_arc1_train = ds_arc1['train'].map(lambda x: add_source_field(x, \"ARC-Challenge\"))\n",
    "ds_arc1_test = ds_arc1['test'].map(lambda x: add_source_field(x, \"ARC-Challenge\"))\n",
    "ds_arc1_validation = ds_arc1['validation'].map(lambda x: add_source_field(x, \"ARC-Challenge\"))\n",
    "\n",
    "ds_arc2_train = ds_arc2['train'].map(lambda x: add_source_field(x, \"ARC-Easy\"))\n",
    "ds_arc2_test = ds_arc2['test'].map(lambda x: add_source_field(x, \"ARC-Easy\"))\n",
    "ds_arc2_validation = ds_arc2['validation'].map(lambda x: add_source_field(x, \"ARC-Easy\"))\n",
    "\n",
    "# Concatenate all dataset splits\n",
    "arc_train_set = concatenate_datasets([ds_arc1_train, ds_arc1_test, ds_arc1_validation,\n",
    "                                      ds_arc2_train, ds_arc2_test, ds_arc2_validation])\n",
    "\n",
    "# Define a mapping for standardizing answer keys\n",
    "answer_key_mapping = {\n",
    "    \"1\": \"A\", \"2\": \"B\", \"3\": \"C\", \"4\": \"D\",  # Convert numbers to letters\n",
    "    \"A\": \"A\", \"B\": \"B\", \"C\": \"C\", \"D\": \"D\", \"E\": \"E\"  # Keep existing valid choices\n",
    "}\n",
    "\n",
    "# Function to normalize `answerKey`\n",
    "def normalize_answer_key(example):\n",
    "    if example[\"answerKey\"] in answer_key_mapping:\n",
    "        example[\"answerKey\"] = answer_key_mapping[example[\"answerKey\"]]\n",
    "    else:\n",
    "        example[\"answerKey\"] = \"UNKNOWN\"  # Handle unexpected values\n",
    "    return example\n",
    "\n",
    "# Apply normalization to the dataset\n",
    "arc_train_set = arc_train_set.map(normalize_answer_key)\n",
    "\n",
    "# Save the new dataset to disk\n",
    "arc_train_set.save_to_disk(\"../datasets/ARC_AI2.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenBookQA\n",
    "\n",
    "https://huggingface.co/datasets/allenai/openbookqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 11914/11914 [00:00<00:00, 535846.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_ob1 = load_dataset(\"allenai/openbookqa\", \"additional\")\n",
    "ds_ob2 = load_dataset(\"allenai/openbookqa\", \"main\")\n",
    "\n",
    "# Add the \"source\" field to each example\n",
    "def add_source_field(example, source_name):\n",
    "    example['source'] = source_name\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to each split in both datasets\n",
    "ds_ob1_train = ds_ob1['train'].map(lambda x: add_source_field(x, \"OpenBook-Additional\"))\n",
    "ds_ob1_test = ds_ob1['test'].map(lambda x: add_source_field(x, \"OpenBook-Additional\"))\n",
    "ds_ob1_validation = ds_ob1['validation'].map(lambda x: add_source_field(x, \"OpenBook-Additional\"))\n",
    "\n",
    "ds_ob2_train = ds_ob2['train'].map(lambda x: add_source_field(x, \"OpenBook-Main\"))\n",
    "ds_ob2_test = ds_ob2['test'].map(lambda x: add_source_field(x, \"OpenBook-Mai\"))\n",
    "ds_ob2_validation = ds_ob2['validation'].map(lambda x: add_source_field(x, \"OpenBook-Mai\"))\n",
    "\n",
    "# Concatenate all dataset splits\n",
    "ob_train_set = concatenate_datasets([ds_ob1_train, ds_ob1_test, ds_ob1_validation,\n",
    "                                     ds_ob2_train, ds_ob2_test, ds_ob2_validation])\n",
    "\n",
    "# Function to update keys and values\n",
    "def update_entry(entry):\n",
    "    return {\n",
    "        \"id\": entry[\"id\"],  \n",
    "        \"question\": entry[\"question_stem\"],\n",
    "        \"choices\": entry[\"choices\"],  \n",
    "        \"answerKey\": entry[\"answerKey\"],\n",
    "        \"source\": entry[\"source\"]\n",
    "    }\n",
    "\n",
    "# Apply transformation\n",
    "updated_data = [update_entry(entry) for entry in ob_train_set]\n",
    "ob_train_set = Dataset.from_list(updated_data)\n",
    "\n",
    "# Save the new dataset to disk\n",
    "ob_train_set.save_to_disk(\"../datasets/OpenBook.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MathQA\n",
    "\n",
    "https://math-qa.github.io/math-QA/\n",
    "\n",
    "JSON files will needed to be converted to HF format with the same labels as the other Q&A datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in MathQA: 37901\n"
     ]
    }
   ],
   "source": [
    "json_dir = \"../datasets/MathQA\"\n",
    "\n",
    "# List to store combined JSON data\n",
    "all_data = []\n",
    "\n",
    "# Loop through all JSON files in the directory\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith(\".json\"):  # Ensure it's a JSON file\n",
    "        file_path = os.path.join(json_dir, filename)\n",
    "        \n",
    "        # Load the JSON file and append its content to all_data\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)  # Each file is a list of dictionaries\n",
    "            all_data.extend(data)  # Concatenate lists\n",
    "\n",
    "print('Number of examples in MathQA:', len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 37901/37901 [00:00<00:00, 448946.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Modify example problems to match labels and formatting for other datasets\n",
    "# Function to transform options into the correct format\n",
    "def parse_options(entry):\n",
    "    # Extract choices using regex\n",
    "    choices = re.findall(r\"[a-e] \\) (.*?)(?= , [a-e] \\) |$)\", entry)\n",
    "    return {\"text\": choices, \"label\": [\"A\", \"B\", \"C\", \"D\", \"E\"]}  \n",
    "       \n",
    "\n",
    "# Function to update keys and values\n",
    "def update_entry(entry):\n",
    "    return {\n",
    "        \"question\": entry[\"Problem\"],  \n",
    "        \"rationale\": entry[\"Rationale\"],\n",
    "        \"choices\": parse_options(entry[\"options\"]),  \n",
    "        \"answerKey\": entry[\"correct\"].upper(),\n",
    "        \"annotatedFormula\": entry[\"annotated_formula\"],\n",
    "        \"linearFormula\": entry[\"linear_formula\"],\n",
    "        \"category\": entry[\"category\"],\n",
    "        \"source\": \"MathQA\"\n",
    "    }\n",
    "\n",
    "# Apply transformation\n",
    "updated_data = [update_entry(entry) for entry in all_data]\n",
    "math_train_set = Dataset.from_list(updated_data)\n",
    "math_train_set.save_to_disk(\"../datasets/MathQA.hf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CommonsenseQA\n",
    "\n",
    "https://huggingface.co/datasets/tau/commonsense_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 10962/10962 [00:00<00:00, 166897.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_cs = load_dataset(\"tau/commonsense_qa\")\n",
    "\n",
    "# Add the \"source\" field to each example\n",
    "def add_source_field(example, source_name):\n",
    "    example['source'] = source_name\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to each split in the dataset\n",
    "ds_cs_train = ds_cs['train'].map(lambda x: add_source_field(x, \"Commonsense\"))\n",
    "ds_cs_validation = ds_cs['validation'].map(lambda x: add_source_field(x, \"Commonsense\"))\n",
    "# ds_cs_test = ds_cs['test'].map(lambda x: add_source_field(x, \"Commonsense\")) # no answerKey provided\n",
    "\n",
    "# Concatenate all dataset splits\n",
    "cs_train_set = concatenate_datasets([ds_cs_train, ds_cs_validation])\n",
    "\n",
    "# Save the new dataset to disk\n",
    "cs_train_set.save_to_disk(\"../datasets/Commonsense.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComSciQA\n",
    "\n",
    "Two versions of this generated dataset: Llama8B and Llama70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 33728/33728 [00:10<00:00, 3306.31 examples/s] \n",
      "Filter: 100%|██████████| 33728/33728 [00:01<00:00, 29841.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 33542/33542 [00:00<00:00, 50381.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "comsci_train_set = load_from_disk(\"../datasets/ComSciQA_Llama70B.hf/train\")\n",
    "\n",
    "# Define a mapping for standardizing answer keys\n",
    "answer_key_mapping = {\n",
    "    \"0\": \"A\", \"1\": \"B\", \"2\": \"C\", \"3\": \"D\",  # Convert numbers to letters\n",
    "    \"A\": \"A\", \"B\": \"B\", \"C\": \"C\", \"D\": \"D\",  # Keep existing valid choices\n",
    "}\n",
    "# Function to normalize `answerKey`\n",
    "def normalize_answer_key(example):\n",
    "    if example[\"answerKey\"] in answer_key_mapping:\n",
    "        example[\"answerKey\"] = answer_key_mapping[example[\"answerKey\"]]\n",
    "    else:\n",
    "        example[\"answerKey\"] = \"UNKNOWN\"  # Handle unexpected values\n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply normalization to the dataset\n",
    "comsci_train_set = comsci_train_set.map(normalize_answer_key).filter(lambda x: x[\"answerKey\"] != \"UNKNOWN\")\n",
    "\n",
    "comsci_train_set.save_to_disk(\"../datasets/Clean_ComSciQA_Llama70B.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComSciQA: 33542 rows\n",
      "CommonsenseQA: 10962 rows\n",
      "MathQA: 37901 rows\n",
      "OpenBookQA: 11914 rows\n",
      "ARC_AI2: 7787 rows\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = {\n",
    "    \"ComSciQA\": comsci_train_set,\n",
    "    \"CommonsenseQA\": cs_train_set,\n",
    "    \"MathQA\": math_train_set,\n",
    "    \"OpenBookQA\": ob_train_set,\n",
    "    \"ARC_AI2\": arc_train_set\n",
    "}\n",
    "\n",
    "for name, dataset in dataset_dict.items():\n",
    "    print(f\"{name}: {dataset.num_rows} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 37901/37901 [00:49<00:00, 767.11 examples/s] \n",
      "Map: 100%|██████████| 11914/11914 [00:05<00:00, 2240.96 examples/s]\n",
      "Filter: 100%|██████████| 102106/102106 [00:07<00:00, 14048.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'choices', 'answer', 'subject'],\n",
      "    num_rows: 93940\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 93940/93940 [00:05<00:00, 17479.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all datasets\n",
    "keys_to_keep = [\"question\", \"choices\", \"answerKey\", \"source\"]\n",
    "\n",
    "# Function to normalize `choices` into `answerChoices`\n",
    "def normalize_choices(example):\n",
    "    if \"choices\" in example and isinstance(example[\"choices\"], dict):\n",
    "        labels = example[\"choices\"].get(\"label\", [])\n",
    "        texts = example[\"choices\"].get(\"text\", [])\n",
    "\n",
    "        answer_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4}\n",
    "\n",
    "        example[\"choices\"] = texts\n",
    "        example[\"answerKey\"] = answer_map.get(example[\"answerKey\"])\n",
    "\n",
    "    return example\n",
    "\n",
    "# Normalize and filter datasets\n",
    "filtered_datasets = []\n",
    "for name, dataset in dataset_dict.items():\n",
    "    # If dataset has multiple splits, take \"train\" split (adjust as needed)\n",
    "    if isinstance(dataset, dict):\n",
    "        dataset = dataset[\"train\"]\n",
    "\n",
    "    # Normalize choices\n",
    "    dataset = dataset.map(normalize_choices)\n",
    "\n",
    "    # Keep only the required columns\n",
    "    filtered_dataset = dataset.select_columns(keys_to_keep)\n",
    "    filtered_datasets.append(filtered_dataset)\n",
    "\n",
    "# Concatenate all filtered datasets\n",
    "combined_train_set = concatenate_datasets(filtered_datasets)\n",
    "combined_train_set = combined_train_set.filter(lambda x: x[\"answerKey\"] != 4)   # MMLU only has 4 answer choices\n",
    "\n",
    "# Rename features to match MMLU\n",
    "combined_train_set = combined_train_set.rename_columns({\"source\": \"subject\", \"answerKey\": \"answer\"})\n",
    "\n",
    "# Print summary\n",
    "print(combined_train_set)\n",
    "combined_train_set.save_to_disk(\"../datasets/CombineQA.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Scientists at a local university have been studying the impact that people have on Earth. One of the areas being studied is how the burning of fossil fuels affects the environment. Which effect of fossil fuel burning have the scientists most likely evaluated?',\n",
       " 'choices': ['the production of nitrogen-fixing bacteria',\n",
       "  'the mechanical weathering of roads',\n",
       "  'the formation of acid rain',\n",
       "  'the increase in runoff'],\n",
       " 'answer': 2,\n",
       " 'subject': 'ARC-Easy'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_set[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 93940/93940 [00:13<00:00, 6895.94 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 3, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unique_answer_keys = combined_train_set.unique(\"answer\")\n",
    "print(unique_answer_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'subject', 'choices', 'answer'],\n",
      "    num_rows: 135\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'A lesion causing compression of the facial nerve at the stylomastoid foramen will cause ipsilateral',\n",
       " 'subject': 'anatomy',\n",
       " 'choices': ['paralysis of the facial muscles.',\n",
       "  'paralysis of the facial muscles and loss of taste.',\n",
       "  'paralysis of the facial muscles, loss of taste and lacrimation.',\n",
       "  'paralysis of the facial muscles, loss of taste, lacrimation and decreased salivation.'],\n",
       " 'answer': 0}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu = load_dataset(\"cais/mmlu\", \"anatomy\", split=\"test\")\n",
    "print(mmlu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
